{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23487\n",
      "18788 4698 18788 4698\n",
      "3870 828\n",
      "15444 3344\n",
      "top 10 words:  [(('fit', 'true', 'size'), 353), (('love', 'love', 'love'), 190), (('cant', 'wait', 'wear'), 179), (('run', 'true', 'size'), 152), (('receive', 'many', 'compliment'), 131), (('size', 'fit', 'perfectly'), 115), (('small', 'fit', 'perfectly'), 108), (('fit', 'like', 'glove'), 104), (('get', 'many', 'compliment'), 93), (('usually', 'wear', 'size'), 92)]\n",
      "vocab_p 395691\n",
      "len_tokensp 455358\n",
      "top 10 words:  [(('really', 'want', 'love'), 66), (('want', 'love', 'dress'), 53), (('make', 'look', 'like'), 48), (('really', 'want', 'like'), 44), (('fit', 'true', 'size'), 29), (('want', 'love', 'top'), 27), (('sadly', 'go', 'back'), 26), (('way', 'much', 'fabric'), 22), (('look', 'like', 'maternity'), 22), (('one', 'go', 'back'), 21)]\n",
      "vocab_n 96709\n",
      "len_tokens_n 102825\n",
      "top 10 words:  [(('fit', 'true', 'size'), 382), (('love', 'love', 'love'), 195), (('cant', 'wait', 'wear'), 179), (('run', 'true', 'size'), 161), (('receive', 'many', 'compliment'), 135), (('size', 'fit', 'perfectly'), 123), (('usually', 'wear', 'size'), 111), (('small', 'fit', 'perfectly'), 111), (('fit', 'like', 'glove'), 109), (('make', 'look', 'like'), 106)]\n",
      "vocab_t 482952\n",
      "[10, 12, 14, 25, 29, 33, 36, 37, 47, 50, 51, 62, 73, 78, 84, 86, 97, 109, 111, 116, 123, 132, 141, 161, 165, 181, 185, 188, 196, 197, 202, 207, 208, 222, 229, 257, 258, 261, 262, 267, 274, 277, 286, 295, 299, 301, 303, 306, 312, 323, 336, 346, 347, 349, 360, 361, 367, 373, 382, 383, 391, 400, 404, 405, 406, 408, 414, 417, 421, 422, 423, 424, 427, 428, 439, 445, 446, 450, 452, 456, 458, 470, 471, 472, 491, 493, 506, 508, 509, 510, 512, 514, 519, 523, 524, 541, 543, 562, 563, 578, 591, 592, 607, 617, 619, 626, 639, 640, 641, 642, 653, 656, 659, 662, 667, 669, 670, 678, 679, 680, 686, 687, 688, 699, 701, 716, 736, 737, 738, 740, 754, 756, 759, 765, 773, 775, 776, 777, 784, 788, 789, 792, 802, 806, 812, 814, 815, 818, 834, 842, 845, 853, 854, 857, 872, 876, 878, 887, 891, 903, 904, 907, 912, 918, 926, 930, 940, 942, 958, 975, 985, 990, 996, 1007, 1011, 1014, 1016, 1031, 1033, 1036, 1040, 1041, 1054, 1074, 1075, 1076, 1084, 1085, 1094, 1098, 1100, 1101, 1109, 1110, 1114, 1117, 1118, 1132, 1136, 1138, 1141, 1142, 1147, 1151, 1157, 1158, 1170, 1201, 1216, 1230, 1249, 1253, 1260, 1265, 1272, 1273, 1275, 1278, 1280, 1286, 1287, 1298, 1302, 1309, 1316, 1317, 1327, 1330, 1331, 1338, 1351, 1355, 1357, 1364, 1378, 1380, 1393, 1407, 1411, 1412, 1415, 1418, 1421, 1430, 1438, 1442, 1444, 1447, 1448, 1452, 1454, 1468, 1469, 1474, 1478, 1484, 1489, 1502, 1506, 1509, 1513, 1514, 1517, 1529, 1540, 1541, 1548, 1552, 1556, 1559, 1575, 1577, 1586, 1588, 1589, 1591, 1594, 1599, 1601, 1602, 1603, 1604, 1607, 1623, 1626, 1627, 1630, 1631, 1639, 1646, 1648, 1651, 1658, 1664, 1676, 1678, 1692, 1696, 1698, 1703, 1706, 1725, 1727, 1734, 1742, 1747, 1748, 1754, 1755, 1758, 1764, 1769, 1773, 1778, 1780, 1782, 1792, 1799, 1807, 1815, 1825, 1826, 1827, 1828, 1833, 1841, 1846, 1850, 1853, 1856, 1860, 1863, 1870, 1880, 1888, 1891, 1900, 1905, 1907, 1908, 1912, 1917, 1922, 1931, 1937, 1953, 1960, 1968, 1973, 1986, 1996, 1997, 1999, 2004, 2008, 2011, 2017, 2029, 2036, 2039, 2042, 2043, 2048, 2051, 2054, 2057, 2058, 2074, 2075, 2079, 2080, 2085, 2087, 2089, 2093, 2099, 2111, 2114, 2118, 2121, 2123, 2134, 2148, 2154, 2157, 2169, 2173, 2179, 2182, 2192, 2199, 2217, 2227, 2233, 2241, 2247, 2253, 2275, 2277, 2278, 2280, 2284, 2285, 2291, 2300, 2303, 2317, 2325, 2330, 2334, 2339, 2346, 2356, 2359, 2371, 2397, 2405, 2408, 2419, 2426, 2430, 2431, 2436, 2439, 2446, 2454, 2459, 2463, 2464, 2473, 2474, 2475, 2476, 2477, 2481, 2488, 2493, 2507, 2512, 2520, 2521, 2523, 2524, 2538, 2551, 2554, 2565, 2586, 2587, 2589, 2592, 2602, 2605, 2612, 2613, 2623, 2628, 2630, 2632, 2634, 2650, 2671, 2673, 2678, 2679, 2684, 2688, 2697, 2704, 2706, 2707, 2730, 2743, 2749, 2759, 2762, 2763, 2764, 2766, 2767, 2769, 2775, 2778, 2782, 2783, 2789, 2791, 2796, 2815, 2821, 2829, 2839, 2840, 2845, 2846, 2847, 2850, 2852, 2859, 2860, 2861, 2866, 2867, 2868, 2876, 2880, 2884, 2917, 2919, 2930, 2937, 2938, 2940, 2949, 2952, 2954, 2968, 2973, 2991, 2992, 2995, 3005, 3007, 3010, 3037, 3038, 3042, 3048, 3051, 3059, 3062, 3067, 3082, 3088, 3089, 3093, 3094, 3097, 3099, 3110, 3122, 3127, 3129, 3134, 3136, 3139, 3143, 3144, 3146, 3147, 3148, 3150, 3157, 3170, 3181, 3182, 3191, 3202, 3204, 3214, 3217, 3219, 3225, 3229, 3233, 3237, 3238, 3244, 3255, 3269, 3272, 3284, 3286, 3291, 3293, 3335, 3337, 3339, 3341, 3345, 3350, 3358, 3382, 3395, 3408, 3433, 3434, 3435, 3445, 3458, 3474, 3475, 3488, 3489, 3496, 3499, 3500, 3501, 3511, 3523, 3528, 3536, 3550, 3553, 3555, 3557, 3559, 3561, 3573, 3574, 3579, 3590, 3593, 3595, 3598, 3616, 3624, 3627, 3641, 3642, 3645, 3651, 3660, 3674, 3675, 3676, 3678, 3681, 3686, 3688, 3694, 3702, 3706, 3707, 3709, 3710, 3722, 3727, 3735, 3739, 3743, 3761, 3774, 3778, 3783, 3785, 3795, 3802, 3808, 3818, 3827, 3845, 3848, 3851, 3853, 3861, 3862, 3869, 3872, 3894, 3904, 3936, 3938, 3949, 3959, 3974, 3977, 3983, 3987, 3991, 4009, 4014, 4017, 4029, 4042, 4043, 4047, 4052, 4064, 4066, 4074, 4080, 4085, 4094, 4106, 4112, 4114, 4134, 4137, 4152, 4163, 4178, 4181, 4184, 4218, 4219, 4222, 4234, 4239, 4240, 4243, 4250, 4253, 4261, 4272, 4274, 4288, 4289, 4300, 4318, 4323, 4331, 4347, 4366, 4375, 4380, 4388, 4396, 4397, 4407, 4413, 4417, 4423, 4426, 4428, 4431, 4440, 4447, 4448, 4451, 4453, 4461, 4462, 4469, 4470, 4472, 4473, 4480, 4487, 4488, 4490, 4494, 4497, 4502, 4525, 4527, 4539, 4542, 4546, 4549, 4561, 4564, 4568, 4569, 4579, 4581, 4593, 4594, 4598, 4599, 4605, 4634, 4645, 4648, 4654, 4675, 4679, 4683, 4692, 4695]\n",
      "770\n",
      "0.8361004682843763\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "import glob\n",
    "import errno\n",
    "import os\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from nltk.corpus import wordnet\n",
    "  \n",
    "data=[]\n",
    "N=3\n",
    "K=1#smoothing\n",
    "v_occur=1\n",
    "exclude = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "\n",
    "def convertTuple(tup): \n",
    "    str =  ''.join(tup) \n",
    "    return str\n",
    "\n",
    "with open('Womens Clothing E-Commerce Reviews.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "         data.append(row)\n",
    "d=np.array(data)\n",
    "print(len(d))\n",
    "\n",
    "#separate classes\n",
    "d=d[1:,1:] #remove first line and column\n",
    "l_recommendation=np.array(d[:,5])\n",
    "title=np.array([d[:,2]])\n",
    "review_text=np.array([d[:,3]])\n",
    "review=np.concatenate((title.T,review_text.T),axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(review, l_recommendation, test_size=0.2,random_state=0)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "c_p=list(y_train).count('1')\n",
    "c_n=list(y_train).count('0')\n",
    "print(list(y_test).count('1'),list(y_test).count('0'))\n",
    "review_p = X_train[y_train == '1', :] # extract all rows with label 1\n",
    "review_n = X_train[y_train == '0', :] # extract all rows with label 0\n",
    "print(len(review_p), len(review_n))\n",
    "\n",
    "\n",
    "def ngram_text(review,N):\n",
    "    text=\"\"\n",
    "    for i in range(len(review)):\n",
    "        txt=str(review[i])\n",
    "        s = ''.join(ch for ch in  txt if ch not in exclude)\n",
    "        out=s.lower()\n",
    "        result = re.sub(r'\\d+', '', out) #remove numbers\n",
    "        text=text+' '+result\n",
    "        \n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))  #tokenizer\n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged) #lemmatizer\n",
    "    lemmas = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:            \n",
    "            lemmas.append(word)\n",
    "        else:\n",
    "            lemmas.append(lemmatizer.lemmatize(word, tag))\n",
    "    filtered_sentence = [w for w in lemmas if not w in stop_words] \n",
    "    \n",
    "    ungs = ngrams(filtered_sentence, 1) #unigram\n",
    "    ufdist = nltk.FreqDist(ungs)    \n",
    "    high_fdist = {convertTuple(k) for k, v in ufdist.items() if v >=v_occur} #remove tokens that occurs more than x times\n",
    "    occurence_sentence = [w for w in filtered_sentence if w in high_fdist] \n",
    "    tokens_final=occurence_sentence\n",
    "    ngs1 = ngrams(tokens_final, N-1)  #n-gram n-1\n",
    "    nfdist1 = nltk.FreqDist(ngs1)\n",
    "    ngs = ngrams(tokens_final, N) #Create n-grams\n",
    "    nfdist = nltk.FreqDist(ngs)    \n",
    "    print(\"top 10 words: \", nfdist.most_common(10))\n",
    "    vocabulary=len(nfdist)    \n",
    "    prob_uni=dict()\n",
    "    for k,v in ufdist.items():\n",
    "        prob_uni[convertTuple(k)]=v\n",
    "    return tokens_final, ngs, ungs, nfdist1, nfdist, vocabulary, prob_uni\n",
    "\n",
    "tokens_p, ngs_p, ungs_p, nfdist1_p, nfdist_p, vocabulary_p, prob_uni_p=ngram_text(review_p,N)\n",
    "print('vocab_p',vocabulary_p)\n",
    "print('len_tokensp',len(tokens_p))\n",
    "tokens_n, ngs_n, ungs_n,nfdist1_n, nfdist_n, vocabulary_n, prob_uni_n=ngram_text(review_n,N)\n",
    "print('vocab_n',vocabulary_n)\n",
    "print('len_tokens_n',len(tokens_n))\n",
    "tokens_t, ngs_t, ungs_t, nfdist1_t, nfdist_t, vocabulary_t, prob_uni_t=ngram_text(X_train,N)\n",
    "print('vocab_t',vocabulary_t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inference=[]\n",
    "\n",
    "for i in range(len(X_test)):  \n",
    "    txt=str(X_test[i])\n",
    "    s = ''.join(ch for ch in  txt if ch not in exclude) #remove punctuation\n",
    "    out=s.lower() #lowercase\n",
    "    result = re.sub(r'\\d+', '', out) #remove numbers\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(result))   #tokenizer\n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged) #lemmatizer\n",
    "    lemmas = []\n",
    "   \n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:     \n",
    "            lemmas.append(word)\n",
    "        else:\n",
    "            lemmas.append(lemmatizer.lemmatize(word, tag))\n",
    "    filtered_sentence = [w for w in lemmas if not w in stop_words] \n",
    "    ungs = ngrams(filtered_sentence, 1) #unigram\n",
    "    ufdist = nltk.FreqDist(ungs)    \n",
    "    high_fdist = {convertTuple(k) for k, v in ufdist.items() if v >=v_occur} #remove tokens that occurs more than x times\n",
    "    occurence_sentence = [w for w in filtered_sentence if w in high_fdist] \n",
    "    Ntotal=0\n",
    "    Ptotal=0\n",
    "    for j in range(len(occurence_sentence)-N+1): \n",
    "        Pcondit=0\n",
    "        Ncondit=0\n",
    "        if (N==1):\n",
    "            tuple1=(occurence_sentence[j])\n",
    "            if tuple1 in tokens_t:\n",
    "                Pcondit=((prob_uni_p.get(tuple1,0)+K)/(len(tokens_p)+(K*vocabulary_t)))\n",
    "                Ncondit=((prob_uni_n.get(tuple1,0)+K)/(len(tokens_n)+(K*vocabulary_t)))                \n",
    "        if (N==2):\n",
    "            if nfdist_t.get(tuple3,0) >0:\n",
    "                Pcondit=((nfdist_p.get(tuple3,0)+K)/(prob_uni_p.get(tuple2,0)+(K*vocabulary_t)))\n",
    "                Ncondit=((nfdist_n.get(tuple3,0)+K)/(prob_uni_n.get(tuple2,0)+(K*vocabulary_t))) \n",
    "        if (N==3):\n",
    "            tuple3a=(occurence_sentence[j],occurence_sentence[j+1],occurence_sentence[j+2])\n",
    "            tuple3b=(occurence_sentence[j],occurence_sentence[j+1])\n",
    "            tuple2a=(occurence_sentence[j+1],occurence_sentence[j+2])\n",
    "            tuple2b=(occurence_sentence[j+1])\n",
    "            tuple1=(occurence_sentence[j+2])\n",
    "            if nfdist_t.get(tuple3a,0)>0:\n",
    "                Pcondit=((nfdist_p.get(tuple3a,0)+K)/(nfdist1_p.get(tuple3b,0)+(K*vocabulary_t)))\n",
    "                Ncondit=((nfdist_n.get(tuple3a,0)+K)/(nfdist1_n.get(tuple3b,0)+(K*vocabulary_t)))\n",
    "        if (N==4):\n",
    "            tuple4a=(occurence_sentence[j],occurence_sentence[j+1],occurence_sentence[j+2],occurence_sentence[j+3])\n",
    "            tuple4b=(occurence_sentence[j],occurence_sentence[j+1],occurence_sentence[j+2])\n",
    "            tuple3a=(occurence_sentence[j+1],occurence_sentence[j+2],occurence_sentence[j+3])\n",
    "            tuple3b=(occurence_sentence[j+1],occurence_sentence[j+2])\n",
    "            tuple2a=(occurence_sentence[j+2],occurence_sentence[j+3])\n",
    "            tuple2b=(occurence_sentence[j+2])\n",
    "            tuple1=(occurence_sentence[j+3])\n",
    "            if nfdist_t.get(tuple4a,0)>0:\n",
    "                Pcondit=((nfdist_p.get(tuple4a,0)+K)/(nfdist1_p.get(tuple4b,0)+(K*vocabulary_t)))\n",
    "                Ncondit=((nfdist_n.get(tuple4a,0)+K)/(nfdist1_n.get(tuple4b,0)+(K*vocabulary_t)))\n",
    "        if Ncondit!=0:\n",
    "            Ntotal+=np.log2(Ncondit)\n",
    "        if Pcondit!=0:\n",
    "            Ptotal+=np.log2(Pcondit)\n",
    "    N_fim=np.log2(c_n/(c_p+c_n))+Ntotal\n",
    "    P_fim=np.log2(c_p/(c_p+c_n))+Ptotal\n",
    "    \n",
    "    if(N_fim>P_fim):\n",
    "        #final=\"N\"\n",
    "        inference.append('0')\n",
    "    else:\n",
    "        #final=\"P\"\n",
    "        inference.append('1') \n",
    "\n",
    "\n",
    "accuracy=0\n",
    "errors=[]\n",
    "for i in range(len(inference)):\n",
    "    if inference[i]==y_test[i]:\n",
    "                         accuracy+=1\n",
    "    else:\n",
    "        errors.append(i)\n",
    "\n",
    "print(errors)\n",
    "print(len(errors))\n",
    "print(accuracy/len(inference))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
