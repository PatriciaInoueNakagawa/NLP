{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23487\n",
      "18788 4698 18788 4698\n",
      "842 1565 2871 5077 13131\n",
      "175 327 566 1007 2623\n",
      "top 10 words u:  [(('look', 'like'), 64), (('going', 'back'), 36), (('looked', 'like'), 35), (('wa', 'excited'), 33), (('poor', 'quality'), 32), (('wanted', 'love'), 25), (('wa', 'really'), 23), (('nothing', 'like'), 23), (('dress', 'wa'), 22), (('wa', 'disappointed'), 22)]\n",
      "top 10 words f:  [(('look', 'nothing', 'like'), 11), (('wa', 'really', 'excited'), 8), (('really', 'wanted', 'love'), 7), (('like', 'wa', 'wearing'), 6), (('wa', 'excited', 'get'), 5), (('really', 'wanted', 'like'), 5), (('first', 'time', 'wore'), 5), (('look', 'cute', 'model'), 5), (('wa', 'excited', 'order'), 5), (('like', 'maternity', 'top'), 5)]\n",
      "vocab_1 19549\n",
      "top 10 words u:  [(('wanted', 'love'), 106), (('look', 'like'), 96), (('looked', 'like'), 83), (('going', 'back'), 80), (('really', 'wanted'), 63), (('made', 'look'), 53), (('dress', 'wa'), 53), (('wa', 'excited'), 51), (('didnt', 'work'), 49), (('wa', 'disappointed'), 49)]\n",
      "top 10 words f:  [(('really', 'wanted', 'love'), 28), (('really', 'wanted', 'like'), 26), (('like', 'wa', 'wearing'), 23), (('wanted', 'love', 'dress'), 17), (('looked', 'like', 'wa'), 14), (('felt', 'like', 'wa'), 13), (('made', 'look', 'like'), 13), (('sadly', 'going', 'back'), 12), (('fit', 'true', 'size'), 12), (('wa', 'excited', 'get'), 10)]\n",
      "vocab_2 38527\n",
      "top 10 words u:  [(('wanted', 'love'), 154), (('look', 'like'), 117), (('run', 'large'), 110), (('run', 'small'), 96), (('dress', 'wa'), 87), (('going', 'back'), 80), (('wa', 'excited'), 79), (('fit', 'well'), 72), (('im', 'lb'), 71), (('really', 'wanted'), 70)]\n",
      "top 10 words f:  [(('wanted', 'love', 'dress'), 35), (('really', 'wanted', 'love'), 32), (('fit', 'true', 'size'), 17), (('felt', 'like', 'wa'), 16), (('wanted', 'love', 'top'), 16), (('wa', 'way', 'big'), 14), (('wa', 'excited', 'receive'), 14), (('made', 'look', 'like'), 13), (('really', 'wanted', 'like'), 13), (('wa', 'really', 'excited'), 13)]\n",
      "vocab_3 72035\n",
      "top 10 words u:  [(('true', 'size'), 209), (('run', 'large'), 180), (('run', 'small'), 178), (('look', 'great'), 151), (('usually', 'wear'), 144), (('fit', 'well'), 123), (('look', 'like'), 123), (('love', 'dress'), 121), (('fit', 'perfectly'), 116), (('size', 'small'), 114)]\n",
      "top 10 words f:  [(('fit', 'true', 'size'), 85), (('run', 'little', 'big'), 33), (('usually', 'wear', 'size'), 26), (('run', 'true', 'size'), 25), (('run', 'little', 'large'), 23), (('ordered', 'usual', 'size'), 21), (('run', 'bit', 'large'), 20), (('cant', 'wait', 'wear'), 20), (('doe', 'run', 'large'), 19), (('size', 'small', 'fit'), 19)]\n",
      "vocab_4 122169\n",
      "top 10 words u:  [(('true', 'size'), 725), (('fit', 'perfectly'), 698), (('love', 'love'), 510), (('love', 'dress'), 507), (('look', 'great'), 506), (('love', 'top'), 374), (('fit', 'great'), 326), (('usually', 'wear'), 303), (('fit', 'well'), 295), (('well', 'made'), 292)]\n",
      "top 10 words f:  [(('fit', 'true', 'size'), 252), (('love', 'love', 'love'), 173), (('cant', 'wait', 'wear'), 159), (('received', 'many', 'compliment'), 109), (('run', 'true', 'size'), 105), (('small', 'fit', 'perfectly'), 93), (('size', 'fit', 'perfectly'), 91), (('fit', 'like', 'glove'), 85), (('dress', 'love', 'dress'), 65), (('usually', 'wear', 'size'), 60)]\n",
      "vocab_5 271136\n",
      "top 10 words u:  [(('true', 'size'), 1038), (('fit', 'perfectly'), 877), (('look', 'great'), 733), (('love', 'dress'), 725), (('look', 'like'), 663), (('love', 'love'), 566), (('run', 'large'), 558), (('usually', 'wear'), 551), (('fit', 'well'), 528), (('love', 'top'), 493)]\n",
      "top 10 words f:  [(('fit', 'true', 'size'), 370), (('love', 'love', 'love'), 193), (('cant', 'wait', 'wear'), 179), (('run', 'true', 'size'), 143), (('received', 'many', 'compliment'), 129), (('size', 'fit', 'perfectly'), 117), (('small', 'fit', 'perfectly'), 110), (('fit', 'like', 'glove'), 108), (('usually', 'wear', 'size'), 106), (('ordered', 'usual', 'size'), 84)]\n",
      "vocab_t 497454\n",
      "4698 4698\n",
      "1997\n",
      "0.5749255002128565\n",
      "1 2 0 4695\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "import glob\n",
    "import errno\n",
    "import os\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "data=[]\n",
    "N=3\n",
    "K=1 #smothing\n",
    "v_occur=1\n",
    "exclude = set(string.punctuation)\n",
    "exclude.union('\\n')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "def convertTuple(tup): \n",
    "    str =  ''.join(tup) \n",
    "    return str\n",
    "\n",
    "with open('Womens Clothing E-Commerce Reviews.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "         data.append(row)\n",
    "d=np.array(data)\n",
    "print(len(d))\n",
    "\n",
    "#separate classes\n",
    "d=d[1:,1:] #remove first line and column\n",
    "l_recommendation=np.array(d[:,4])\n",
    "title=np.array([d[:,2]])\n",
    "review_text=np.array([d[:,3]])\n",
    "review=np.concatenate((title.T,review_text.T),axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(review, l_recommendation, test_size=0.2,random_state=0)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "print(list(l_recommendation).count('1'),list(l_recommendation).count('2'),list(l_recommendation).count('3'),list(l_recommendation).count('4'),list(l_recommendation).count('5'))\n",
    "print(list(y_test).count('1'),list(y_test).count('2'),list(y_test).count('3'),list(y_test).count('4'),list(y_test).count('5'))\n",
    "c_1=list(y_train).count('1')\n",
    "c_2=list(y_train).count('2')\n",
    "c_3=list(y_train).count('3')\n",
    "c_4=list(y_train).count('4')\n",
    "c_5=list(y_train).count('5')\n",
    "c_total=c_1+c_2+c_3+c_4+c_5\n",
    "\n",
    "review_1 = X_train[y_train == '1', :] # extract all rows with label 1\n",
    "review_2 = X_train[y_train == '2', :] # extract all rows with label 2\n",
    "review_3 = X_train[y_train == '3', :] # extract all rows with label 3\n",
    "review_4 = X_train[y_train == '4', :] # extract all rows with label 4\n",
    "review_5 = X_train[y_train == '5', :] # extract all rows with label 5\n",
    "\n",
    "\n",
    "def ngram_text(review,N):\n",
    "    text=\"\"\n",
    "    for i in range(len(review)):\n",
    "        txt=str(review[i])\n",
    "        s = ''.join(ch for ch in  txt if ch not in exclude)\n",
    "        out=s.lower()\n",
    "        result = re.sub(r'\\d+', '', out) #remove numbers\n",
    "        text=text+\" \"+result\n",
    "        \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmas= [lemmatizer.lemmatize(tokens[t]) for t in range(len(tokens))]\n",
    "    filtered_sentence = [w for w in lemmas if not w in stop_words] \n",
    "    ungs = ngrams(filtered_sentence, N-1) #ngram-1\n",
    "    ufdist = nltk.FreqDist(ungs)\n",
    "    \n",
    "    print(\"top 10 words u: \", ufdist.most_common(10))\n",
    "    high_fdist = {convertTuple(k) for k, v in ufdist.items() if v >=v_occur} #remove tokens that occurs more than x times\n",
    "    occurence_sentence = [w for w in filtered_sentence if w in high_fdist] \n",
    "    tokens_final=occurence_sentence\n",
    "    \n",
    "    ngs = ngrams(tokens_final, N) #Create n-grams\n",
    "    nfdist = nltk.FreqDist(ngs)    \n",
    "    \n",
    "    print(\"top 10 words f: \", nfdist.most_common(10))\n",
    "    vocabulary=len(nfdist)    \n",
    "    prob=dict()\n",
    "    for k,v in ufdist.items():\n",
    "        prob[convertTuple(k)]=v\n",
    "    \n",
    "    return tokens_final, ngs, ungs, ufdist, nfdist, vocabulary, prob, high_fdist\n",
    "\n",
    "\n",
    "tokens_1, ngs_1, ungs_1, ufdist_1, nfdist_1, vocabulary_1, prob_1, high_fdist_1=ngram_text(review_1,N)\n",
    "print('vocab_1',vocabulary_1)\n",
    "tokens_2, ngs_2, ungs_2, ufdist_2, nfdist_2, vocabulary_2, prob_2, high_fdist_2=ngram_text(review_2,N)\n",
    "print('vocab_2',vocabulary_2)\n",
    "tokens_3, ngs_3, ungs_3,ufdist_3, nfdist_3, vocabulary_3, prob_3, high_fdist_3=ngram_text(review_3,N)\n",
    "print('vocab_3',vocabulary_3)\n",
    "tokens_4, ngs_4, ungs_4, ufdist_4, nfdist_4, vocabulary_4, prob_4, high_fdist_4=ngram_text(review_4,N)\n",
    "print('vocab_4',vocabulary_4)\n",
    "tokens_5, ngs_5, ungs_5, ufdist_5, nfdist_5, vocabulary_5, prob_5, high_fdist_5=ngram_text(review_5,N)\n",
    "print('vocab_5',vocabulary_5)\n",
    "tokens_t, ngs_t, ungs_t, ufdist_t, nfdist_t, vocabulary_t, prob_t, high_fdist_t=ngram_text(X_train,N)\n",
    "print('vocab_t',vocabulary_t)\n",
    "\n",
    "\n",
    "inference=[]\n",
    "for i in range(len(X_test)):  \n",
    "    txt=str(X_test[i])\n",
    "    s = ''.join(ch for ch in  txt if ch not in exclude)\n",
    "    out=s.lower()\n",
    "    result = re.sub(r'\\d+', '', out) #remove numbers\n",
    "    tokens =nltk.word_tokenize(result)\n",
    "    lemmas= [lemmatizer.lemmatize(tokens[t]) for t in range(len(tokens))]\n",
    "    filtered_sentence = [w for w in lemmas if not w in stop_words] \n",
    "    occurence_sentence = [w for w in filtered_sentence if w in high_fdist_t] \n",
    "    total_1=0\n",
    "    total_2=0\n",
    "    total_3=0\n",
    "    total_4=0\n",
    "    total_5=0\n",
    "    for j in range(len(occurence_sentence)-N+1):\n",
    "        condit_1=0\n",
    "        condit_2=0\n",
    "        condit_3=0\n",
    "        condit_4=0\n",
    "        condit_5=0\n",
    "        \n",
    "        if(N==4):\n",
    "            thistuple3=(occurence_sentence[j],occurence_sentence[j+1],occurence_sentence[j+2],occurence_sentence[j+3])\n",
    "            thistuple2=(occurence_sentence[j],occurence_sentence[j+1],occurence_sentence[j+2])\n",
    "            condit_1=(nfdist_1.get(thistuple3,0)+K)/(ufdist_1.get(thistuple2,0)+(K*vocabulary_t))\n",
    "            condit_2=(nfdist_2.get(thistuple3,0)+K)/(ufdist_2.get(thistuple2,0)+(K*vocabulary_t))\n",
    "            condit_3=(nfdist_3.get(thistuple3,0)+K)/(ufdist_3.get(thistuple2,0)+(K*vocabulary_t))\n",
    "            condit_4=(nfdist_4.get(thistuple3,0)+K)/(ufdist_4.get(thistuple2,0)+(K*vocabulary_t))\n",
    "            condit_5=(nfdist_5.get(thistuple3,0)+K)/(ufdist_5.get(thistuple2,0)+(K*vocabulary_t))\n",
    "        if(N==3):\n",
    "            thistuple3=(occurence_sentence[j],occurence_sentence[j+1],occurence_sentence[j+2])\n",
    "            thistuple2=(occurence_sentence[j],occurence_sentence[j+1])\n",
    "            if nfdist_t.get(thistuple3,0)>0:\n",
    "                condit_1=(nfdist_1.get(thistuple3,0)+K)/(ufdist_1.get(thistuple2,0)+(K*vocabulary_t))\n",
    "                condit_2=(nfdist_2.get(thistuple3,0)+K)/(ufdist_2.get(thistuple2,0)+(K*vocabulary_t))\n",
    "                condit_3=(nfdist_3.get(thistuple3,0)+K)/(ufdist_3.get(thistuple2,0)+(K*vocabulary_t))\n",
    "                condit_4=(nfdist_4.get(thistuple3,0)+K)/(ufdist_4.get(thistuple2,0)+(K*vocabulary_t))\n",
    "                condit_5=(nfdist_5.get(thistuple3,0)+K)/(ufdist_5.get(thistuple2,0)+(K*vocabulary_t))           \n",
    "        if(N==2):\n",
    "            thistuple3=(occurence_sentence[j],occurence_sentence[j+1])\n",
    "            thistuple2=(occurence_sentence[j])\n",
    "            if nfdist_t.get(thistuple3,0)>0:\n",
    "                condit_1=(nfdist_1.get(thistuple3,0)+K)/(prob_1.get(thistuple2,0)+(K*vocabulary_t))\n",
    "                condit_2=(nfdist_2.get(thistuple3,0)+K)/(prob_2.get(thistuple2,0)+(K*vocabulary_t))\n",
    "                condit_3=(nfdist_3.get(thistuple3,0)+K)/(prob_3.get(thistuple2,0)+(K*vocabulary_t))\n",
    "                condit_4=(nfdist_4.get(thistuple3,0)+K)/(prob_4.get(thistuple2,0)+(K*vocabulary_t))\n",
    "                condit_5=(nfdist_5.get(thistuple3,0)+K)/(prob_5.get(thistuple2,0)+(K*vocabulary_t))\n",
    "        \n",
    "        if(N==1):\n",
    "            thistuple2=occurence_sentence[j]\n",
    "            if prob_t.get(occurence_sentence[j],0)>0:\n",
    "                condit_1=(prob_1.get(occurence_sentence[j],0)+K)/(len(tokens_1)+(K*vocabulary_t))\n",
    "                condit_2=(prob_2.get(occurence_sentence[j],0)+K)/(len(tokens_2)+(K*vocabulary_t))\n",
    "                condit_3=(prob_3.get(occurence_sentence[j],0)+K)/(len(tokens_3)+(K*vocabulary_t))\n",
    "                condit_4=(prob_4.get(occurence_sentence[j],0)+K)/(len(tokens_4)+(K*vocabulary_t))\n",
    "                condit_5=(prob_5.get(occurence_sentence[j],0)+K)/(len(tokens_5)+(K*vocabulary_t))\n",
    "        \n",
    "        if condit_1>0:\n",
    "            total_1+=np.log2(condit_1)\n",
    "        if condit_2>0:\n",
    "            total_2+=np.log2(condit_2)\n",
    "        if condit_3>0:\n",
    "            total_3+=np.log2(condit_3)\n",
    "        if condit_4>0:\n",
    "            total_4+=np.log2(condit_4)\n",
    "        if condit_5>0:\n",
    "            total_5+=np.log2(condit_5)\n",
    "    \n",
    "    total_1_fim=np.log2(c_1/c_total)+total_1\n",
    "    total_2_fim=np.log2(c_2/c_total)+total_2\n",
    "    total_3_fim=np.log2(c_3/c_total)+total_3\n",
    "    total_4_fim=np.log2(c_4/c_total)+total_4\n",
    "    total_5_fim=np.log2(c_5/c_total)+total_5\n",
    "    get_max=max(total_1_fim, total_2_fim, total_3_fim, total_4_fim, total_5_fim) \n",
    "    \n",
    "    if(get_max==total_1_fim):\n",
    "        inference.append('1')\n",
    "    elif(get_max==total_2_fim):\n",
    "        inference.append('2')\n",
    "    elif(get_max==total_3_fim):\n",
    "        inference.append('3') \n",
    "    elif(get_max==total_4_fim):\n",
    "        inference.append('4')\n",
    "    elif(get_max==total_5_fim):\n",
    "        inference.append('5')\n",
    "    else:\n",
    "        inference.append('0')\n",
    "        print(i,total_1_fim, total_2_fim, total_3_fim, total_4_fim, total_5_fim) \n",
    "    \n",
    "\n",
    "\n",
    "print(len(inference), len(y_test))       \n",
    "accuracy=0\n",
    "errors=[]\n",
    "for i in range(len(inference)):\n",
    "    if inference[i]==y_test[i]:\n",
    "                         accuracy+=1\n",
    "    else:\n",
    "        errors.append(i)\n",
    "\n",
    "print(len(errors))\n",
    "print(accuracy/len(inference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
